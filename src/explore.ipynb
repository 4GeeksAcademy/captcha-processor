{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Explore here"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'!pip install ultralytics'"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "'''!pip install ultralytics'''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'!pip install opencv-python'"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "'''!pip install opencv-python'''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'pip install opendatasets'"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "'''pip install opendatasets'''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import requests\n",
                "from pickle import dump\n",
                "import os\n",
                "import opendatasets as od\n",
                "import zipfile\n",
                "import tensorflow as tf\n",
                "from keras.preprocessing import image\n",
                "from pathlib import Path\n",
                "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
                "from tensorflow.keras.optimizers import Adam, SGD\n",
                "# import keras_tuner as kt\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "import shutil\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten, MaxPooling2D\n",
                "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
                "from keras.models import load_model\n",
                "from keras.losses import *\n",
                "from tensorflow.keras import *\n",
                "import random\n",
                "import hashlib\n",
                "import cv2\n",
                "from matplotlib import pyplot as plt\n",
                "import re\n",
                "import ultralytics\n",
                "from IPython.display import clear_output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First of all, we download the files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [],
            "source": [
                "# # Assign the Kaggle data set URL into variable\n",
                "# dataset = \"https://www.kaggle.com/datasets/youthamj/captchaobjectdetection\"\n",
                "# # Using opendatasets let's download the data sets\n",
                "# od.download(dataset, data_dir=\"../data/raw/\", force=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we calculate the number of files donwloaded."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "200002\n"
                    ]
                }
            ],
            "source": [
                "data_dir = \"../data/raw/captchaobjectdetection\"\n",
                "\n",
                "print(len([file for file in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, file))]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then, we calculate the number of images (.png extension)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "100000\n"
                    ]
                }
            ],
            "source": [
                "im_count = 0\n",
                "\n",
                "for im in Path(data_dir).glob(\"*.png\"):\n",
                "    im_count+=1\n",
                "\n",
                "print(im_count)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can observe that the number of images and files differ by one since there are 200,001 files in total and 100,000 images. Therefore, we have one extra file aside from those associated with each image.\n",
                "\n",
                "The extra file is all_sequences.txt, that contains the relationship between the name of the image/file and the characters that contains.\n",
                "\n",
                "We will use this file to asociate the characters in each row to the number asociated to it in the YOLO files."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The content of the files could be duplicated since there could be two different images with the same characters to decipher, so we only check there are no duplicates in our images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "# def hashfile(file_path):\n",
                "#     hasher = hashlib.sha256()\n",
                "#     with open(file_path, 'rb') as f:\n",
                "#         buf = f.read()\n",
                "#         hasher.update(buf)\n",
                "#     return hasher.hexdigest()\n",
                "\n",
                "\n",
                "# hashes = pd.DataFrame(columns=[\"filename\",\"hash\"])\n",
                "# hashes_dup = pd.DataFrame(columns=[\"filename\",\"hash\"])\n",
                "\n",
                "# for filename in Path(data_dir).glob(\"*.png\"):\n",
                "#   if filename.is_file():\n",
                "#     file_hash = hashfile(filename)\n",
                "#     if (hashes[\"hash\"] != file_hash).all():\n",
                "#       hashes.loc[len(hashes)] = [filename, file_hash]\n",
                "#     else:\n",
                "#       hashes_dup.loc[len(hashes_dup)] = [filename, file_hash]\n",
                "\n",
                "# if hashes_dup.empty:\n",
                "#   print(\"No duplicates in the dataset.\")\n",
                "# else:\n",
                "#     print(hashes_dup)\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's check then an inmage, and it's bounding boxes, to see everything goes as expected in the YOLO files."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we check as an example a random image and its bounding box, to check the values are defined as expected."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def plot_img(img_dir, txt_dir):\n",
                "    image = cv2.imread(img_dir)\n",
                "    image_hight = image.shape[0]\n",
                "    image_width = image.shape[1]\n",
                "\n",
                "    filename = txt_dir\n",
                "\n",
                "    with open(filename, 'r', encoding='utf-8') as f:\n",
                "        text_list = [list(map(float, line.strip().split())) for line in f]\n",
                "\n",
                "\n",
                "    for i in range(len(text_list)):\n",
                "        x0 = text_list[i][1] - text_list[1][3] / 2\n",
                "        x1 = text_list[i][1] + text_list[i][3] / 2\n",
                "        y0 = text_list[i][2] - text_list[i][4] / 2\n",
                "        y1 = text_list[i][2] + text_list[i][4] / 2\n",
                "\n",
                "        start_point = (int(x0*image_width), int(y0*image_hight))\n",
                "        end_point = (int(x1*image_width), int(y1*image_hight))\n",
                "\n",
                "        img = cv2.rectangle(image, start_point, end_point, color=(255, 0, 0), thickness=2)\n",
                "\n",
                "    plt.imshow(img)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "img_dir = \"../data/raw/captchaobjectdetection/10084.png\"\n",
                "\n",
                "txt_dir = \"../data/raw/captchaobjectdetection/10084.txt\"\n",
                "\n",
                "plot_img(img_dir, txt_dir)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, it's time to create the folders to divide the files in txt and images, and then split then in 3 subsets of train, validation and test, in a rate of 60-20-20."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(\"../data/raw/images\")\n",
                "os.makedirs(\"../data/raw/labels\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {},
            "outputs": [],
            "source": [
                "for im in Path(\"C:/Users/tisor/Projects/captcha-processor/captcha-processor/data/raw/captchaobjectdetection\").glob(\"*.png\"):\n",
                "    shutil.move(im, os.path.join(\"../data/raw/images\", im.name))\n",
                "\n",
                "for im in Path(\"C:/Users/tisor/Projects/captcha-processor/captcha-processor/data/raw/captchaobjectdetection\").glob(\"*.txt\"):\n",
                "    shutil.move(im, os.path.join(\"../data/raw/labels\", im.name))\n",
                "\n",
                "for im in Path(\"../data/raw/labels\").glob(\"all_sequences.txt\"):\n",
                "    shutil.move(im, os.path.join(\"C:/Users/tisor/Projects/captcha-processor/captcha-processor/data/raw/captchaobjectdetection\", im.name))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(\"../data/raw/images/train\")\n",
                "os.makedirs(\"../data/raw/images/val\")\n",
                "os.makedirs(\"../data/raw/images/test\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.makedirs(\"../data/raw/labels/train\")\n",
                "os.makedirs(\"../data/raw/labels/val\")\n",
                "os.makedirs(\"../data/raw/labels/test\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "origin = \"../data/raw/images\"\n",
                "destiny_train = \"../data/raw/images/train\"\n",
                "destiny_val = \"../data/raw/images/val\"\n",
                "destiny_test = \"../data/raw/images/test\"\n",
                "\n",
                "files = [f for f in os.listdir(origin) if os.path.isfile(os.path.join(origin, f))]\n",
                "    \n",
                "size_to_move = int(len(files) * (20 / 100))\n",
                "    \n",
                "files_to_move_val = random.sample(files, size_to_move)\n",
                "\n",
                "for file in files_to_move_val:\n",
                "    shutil.move(os.path.join(origin, file), os.path.join(destiny_val, file))\n",
                "\n",
                "files = [f for f in os.listdir(origin) if os.path.isfile(os.path.join(origin, f))]\n",
                "files_to_move_test = random.sample(files, size_to_move)\n",
                "\n",
                "for file in files_to_move_test:\n",
                "    shutil.move(os.path.join(origin, file), os.path.join(destiny_test, file))\n",
                "\n",
                "for im in Path(\"../data/raw/images\").glob(\"*.png\"):\n",
                "    shutil.move(im, os.path.join(\"../data/raw/images/train\", im.name))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "destiny_train_im = \"../data/raw/images/train\"\n",
                "destiny_val_im = \"../data/raw/images/val\"\n",
                "destiny_test_im = \"../data/raw/images/test\"\n",
                "\n",
                "origin_txt = \"../data/raw/labels\"\n",
                "destiny_train_txt = \"../data/raw/labels/train\"\n",
                "destiny_val_txt = \"../data/raw/labels/val\"\n",
                "destiny_test_txt = \"../data/raw/labels/test\"\n",
                "\n",
                "im_train = {os.path.splitext(f)[0] for f in os.listdir(destiny_train_im) if os.path.isfile(os.path.join(destiny_train_im, f))}\n",
                "im_val = {os.path.splitext(f)[0] for f in os.listdir(destiny_val_im) if os.path.isfile(os.path.join(destiny_val_im, f))}\n",
                "im_test = {os.path.splitext(f)[0] for f in os.listdir(destiny_test_im) if os.path.isfile(os.path.join(destiny_test_im, f))}\n",
                "\n",
                "for file in os.listdir(origin_txt):\n",
                "    path = os.path.join(origin_txt, file)\n",
                "\n",
                "    if os.path.isfile(path):\n",
                "        root, _ = os.path.splitext(file)\n",
                "\n",
                "        # Si el nombre coincide con los nombres de la carpeta de imágenes, moverlo\n",
                "        if root in im_train:\n",
                "            shutil.move(path, os.path.join(destiny_train_txt, file))\n",
                "        \n",
                "        elif root in im_val:\n",
                "            shutil.move(path, os.path.join(destiny_val_txt, file))\n",
                "\n",
                "        elif root in im_test:\n",
                "            shutil.move(path, os.path.join(destiny_test_txt, file))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's check the folders have the number of files expected."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "60000\n",
                        "20000\n",
                        "20000\n",
                        "60000\n",
                        "20000\n",
                        "20000\n"
                    ]
                }
            ],
            "source": [
                "print(len(os.listdir(destiny_train_im)))\n",
                "print(len(os.listdir(destiny_val_im)))\n",
                "print(len(os.listdir(destiny_test_im)))\n",
                "\n",
                "print(len(os.listdir(destiny_train_txt)))\n",
                "print(len(os.listdir(destiny_val_txt)))\n",
                "print(len(os.listdir(destiny_test_txt)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Meanwhile, we've been analyzing the file all_sequences.txt, with the characters associated to each image, and compare them with every txt file associated to each image, to obtain the YOLO yaml index associated to each character.\n",
                "\n",
                "We noticed that there are some characters missing, so we check this characters don't exist in any file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[]\n"
                    ]
                }
            ],
            "source": [
                "filename = \"../data/raw/captchaobjectdetection/all_sequences.txt\"\n",
                "\n",
                "with open(filename, 'r', encoding='utf-8') as f:\n",
                "  char_list = [linea.strip().split(\",\") for linea in f]\n",
                "\n",
                "data = []\n",
                "for item in char_list:\n",
                "   if re.search(r\"iIlLoO01\", item[1]):\n",
                "    data.append(item)\n",
                "\n",
                "print(data)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's begin with the YOLO model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ultralytics import YOLO\n",
                "model = YOLO(\"yolov8n.pt\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "img_dir = \"../data/raw/images/train/3.png\"\n",
                "image = cv2.imread(img_dir)\n",
                "image_size = image.size"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First of all, we train the model with our dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ultralytics 8.3.82  Python-3.9.13 torch-2.6.0+cpu CPU (AMD Ryzen 7 7435HS)\n",
                        "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=./dataset.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=28800, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train7\n",
                        "Overriding model.yaml nc=80 with nc=54\n",
                        "\n",
                        "                   from  n    params  module                                       arguments                     \n",
                        "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
                        "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
                        "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
                        "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
                        "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
                        "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
                        "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
                        "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
                        "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
                        "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
                        " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                        " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
                        " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                        " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
                        " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
                        " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
                        " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
                        " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
                        " 22        [15, 18, 21]  1    761842  ultralytics.nn.modules.head.Detect           [54, [64, 128, 256]]          \n",
                        "Model summary: 129 layers, 3,021,378 parameters, 3,021,362 gradients, 8.3 GFLOPs\n",
                        "\n",
                        "Transferred 319/355 items from pretrained weights\n",
                        "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train7', view at http://localhost:6006/\n",
                        "Freezing layer 'model.22.dfl.conv.weight'\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\tisor\\Projects\\captcha-processor\\captcha-processor\\data\\raw\\labels\\train.cache... 60000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 60000/60000 [00:00<?, ?it/s]\n",
                        "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\tisor\\Projects\\captcha-processor\\captcha-processor\\data\\raw\\labels\\val.cache... 20000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20000/20000 [00:00<?, ?it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Plotting labels to runs\\detect\\train7\\labels.jpg... \n",
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
                        "\u001b[34m\u001b[1mTensorBoard: \u001b[0mWARNING  TensorBoard graph visualization failure [enforce fail at alloc_cpu.cpp:115] data. DefaultCPUAllocator: not enough memory: you tried to allocate 13271040000 bytes.\n",
                        "Image sizes 28800 train, 28800 val\n",
                        "Using 0 dataloader workers\n",
                        "Logging results to \u001b[1mruns\\detect\\train7\u001b[0m\n",
                        "Starting training for 50 epochs...\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "  0%|          | 0/3750 [00:04<?, ?it/s]\n"
                    ]
                },
                {
                    "ename": "error",
                    "evalue": "OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\imgwarp.cpp:1813: error: (-215:Assertion failed) dst.cols < SHRT_MAX && dst.rows < SHRT_MAX && src.cols < SHRT_MAX && src.rows < SHRT_MAX in function 'cv::remap'\n",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dataset.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ruta al archivo de configuración del dataset\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Número de épocas de entrenamiento\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tamaño de las imágenes\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tamaño del batch\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\model.py:810\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\trainer.py:363\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    361\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m TQDM(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader), total\u001b[38;5;241m=\u001b[39mnb)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_train_batch_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;66;03m# Warmup\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\data\\build.py:48\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a sampler that repeats indefinitely.\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\data\\base.py:288\u001b[0m, in \u001b[0;36mBaseDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns transformed label information for given index.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_and_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\data\\augment.py:201\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mApplies a series of transformations to input data. This method sequentially applies each transformation in the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03mCompose object's list of transforms to the input data.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m    >>> transformed_data = compose(input_data)\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m--> 201\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\data\\augment.py:201\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03mApplies a series of transformations to input data. This method sequentially applies each transformation in the\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;124;03mCompose object's list of transforms to the input data.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;124;03m    >>> transformed_data = compose(input_data)\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m--> 201\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\data\\augment.py:1235\u001b[0m, in \u001b[0;36mRandomPerspective.__call__\u001b[1;34m(self, labels)\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m border[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m border[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# w, h\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;66;03m# M is affine matrix\u001b[39;00m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;66;03m# Scale for func:`box_candidates`\u001b[39;00m\n\u001b[1;32m-> 1235\u001b[0m img, M, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mborder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1237\u001b[0m bboxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_bboxes(instances\u001b[38;5;241m.\u001b[39mbboxes, M)\n\u001b[0;32m   1239\u001b[0m segments \u001b[38;5;241m=\u001b[39m instances\u001b[38;5;241m.\u001b[39msegments\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\data\\augment.py:1077\u001b[0m, in \u001b[0;36mRandomPerspective.affine_transform\u001b[1;34m(self, img, border)\u001b[0m\n\u001b[0;32m   1075\u001b[0m         img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwarpPerspective(img, M, dsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, borderValue\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m114\u001b[39m, \u001b[38;5;241m114\u001b[39m, \u001b[38;5;241m114\u001b[39m))\n\u001b[0;32m   1076\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# affine\u001b[39;00m\n\u001b[1;32m-> 1077\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwarpAffine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mborderValue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m114\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m114\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m114\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img, M, s\n",
                        "\u001b[1;31merror\u001b[0m: OpenCV(4.11.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\imgwarp.cpp:1813: error: (-215:Assertion failed) dst.cols < SHRT_MAX && dst.rows < SHRT_MAX && src.cols < SHRT_MAX && src.rows < SHRT_MAX in function 'cv::remap'\n"
                    ]
                }
            ],
            "source": [
                "results = model.train(\n",
                "    data=\"./dataset.yaml\",  # Ruta al archivo de configuración del dataset\n",
                "    epochs=50,  # Número de épocas de entrenamiento\n",
                "    imgsz=image_size,  # Tamaño de las imágenes\n",
                "    batch=16  # Tamaño del batch\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "When we try to train the model, we can see the following error:\n",
                "\n",
                "val: WARNING  C:\\4Geeks\\Projects\\captcha-processor\\data\\raw\\images\\val\\9998.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0333]\n",
                "train: WARNING  C:\\4Geeks\\Projects\\captcha-processor\\data\\raw\\images\\train\\99960.png: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0333]\n",
                "\n",
                "It seems that thousands of the YOLO files have at least, one value bigger than 1, wich is not valid, due to value normalization.\n",
                "\n",
                "Let's copy some of this images and txt in the original folder, soy we can see whats happenning, and what can we do."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'plot_img' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m img_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/raw/images/train/10006.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m txt_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/raw/labels/train/10006.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mplot_img\u001b[49m(img_dir, txt_dir)\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'plot_img' is not defined"
                    ]
                }
            ],
            "source": [
                "img_dir = \"../data/raw/images/train/10006.png\"\n",
                "txt_dir = \"../data/raw/labels/train/10006.txt\"\n",
                "\n",
                "plot_img(img_dir, txt_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's compare the results whith the YOLO file normalized (replacing any value bigger than one, with one)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 640x480 with 1 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# img_dir = \"../data/interim/10193.png\"\n",
                "# txt_dir = \"../data/interim/10193_normalized.txt\"\n",
                "\n",
                "# plot_img(img_dir, txt_dir)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can notice that the rectangles stay the same, but the values are now valid in YOLO files, so they have no values outside 0 to 1."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "So, the key now is to replace every value bigger than one, to one, in every text file all along the YOLO files."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First of all in this step is to create a back up of the original txt."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'../data/raw/captchaobjectdetection'"
                        ]
                    },
                    "execution_count": 49,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "shutil.copytree(\"../data/raw/labels/\", \"../data/raw/captchaobjectdetection\", dirs_exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And then, modify the files to normalize values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [],
            "source": [
                "dir = \"../data/raw/labels\"\n",
                "\n",
                "for file in Path(dir).rglob(\"*.txt\"):   \n",
                "    with open(file, \"r+\", encoding=\"utf-8\") as f:\n",
                "        lines = f.readlines()\n",
                "        f.seek(0)  \n",
                "        for line in lines:\n",
                "            values = list(map(float, line.strip().split()))\n",
                "            values = [1 if x > 1 else x for x in values]\n",
                "            f.write(\" \".join(map(str, values)) + \"\\n\")\n",
                "        f.truncate() "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's try again the YOLO training, in order to see if our changes normalizing the txt file values have solved the problem."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Ultralytics 8.3.82  Python-3.9.13 torch-2.6.0+cpu CPU (AMD Ryzen 7 7435HS)\n",
                        "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=./dataset.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train9\n",
                        "\n",
                        "                   from  n    params  module                                       arguments                     \n",
                        "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
                        "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
                        "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
                        "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
                        "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
                        "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
                        "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
                        "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
                        "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
                        "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
                        " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                        " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
                        " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
                        " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
                        " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
                        " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
                        " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
                        " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
                        " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
                        " 22        [15, 18, 21]  1    761842  ultralytics.nn.modules.head.Detect           [54, [64, 128, 256]]          \n",
                        "Model summary: 129 layers, 3,021,378 parameters, 3,021,362 gradients, 8.3 GFLOPs\n",
                        "\n",
                        "Transferred 355/355 items from pretrained weights\n",
                        "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train9', view at http://localhost:6006/\n",
                        "Freezing layer 'model.22.dfl.conv.weight'\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\tisor\\Projects\\captcha-processor\\captcha-processor\\data\\raw\\labels\\train.cache... 60000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 60000/60000 [00:00<?, ?it/s]\n",
                        "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\tisor\\Projects\\captcha-processor\\captcha-processor\\data\\raw\\labels\\val.cache... 20000 images, 0 backgrounds, 0 corrupt: 100%|██████████| 20000/20000 [00:00<?, ?it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Plotting labels to runs\\detect\\train9\\labels.jpg... \n",
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
                        "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
                        "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
                        "Image sizes 640 train, 640 val\n",
                        "Using 0 dataloader workers\n",
                        "Logging results to \u001b[1mruns\\detect\\train9\u001b[0m\n",
                        "Starting training for 50 epochs...\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       1/50         0G      1.007     0.9407      1.156        214        640: 100%|██████████| 3750/3750 [3:31:05<00:00,  3.38s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [08:45<00:00,  1.19it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153      0.994      0.989      0.995      0.842\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       2/50         0G       0.79     0.5262      1.032        228        640: 100%|██████████| 3750/3750 [3:09:13<00:00,  3.03s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [09:01<00:00,  1.15it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153      0.999      0.998      0.995      0.883\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       3/50         0G       0.74     0.4693      1.007        226        640: 100%|██████████| 3750/3750 [3:09:05<00:00,  3.03s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [08:48<00:00,  1.18it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1      0.999      0.995        0.9\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       4/50         0G     0.6851     0.4075     0.9828        229        640: 100%|██████████| 3750/3750 [3:14:24<00:00,  3.11s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [12:32<00:00,  1.20s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1      0.999      0.995      0.925\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       5/50         0G      0.627     0.3597     0.9591        246        640: 100%|██████████| 3750/3750 [3:18:25<00:00,  3.17s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [12:31<00:00,  1.20s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1          1      0.995      0.946\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       6/50         0G     0.5876     0.3359     0.9435        223        640: 100%|██████████| 3750/3750 [3:59:07<00:00,  3.83s/it]    \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [12:34<00:00,  1.21s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1          1      0.995      0.953\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       7/50         0G       0.56      0.321     0.9331        209        640: 100%|██████████| 3750/3750 [3:24:28<00:00,  3.27s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [12:21<00:00,  1.19s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1          1      0.995      0.963\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       8/50         0G     0.5409     0.3102      0.926        254        640: 100%|██████████| 3750/3750 [3:51:50<00:00,  3.71s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [08:41<00:00,  1.20it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1          1      0.995      0.971\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "       9/50         0G     0.5243     0.3015     0.9199        225        640: 100%|██████████| 3750/3750 [4:14:49<00:00,  4.08s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [12:28<00:00,  1.20s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1          1      0.995      0.972\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      10/50         0G      0.512     0.2951     0.9156        248        640: 100%|██████████| 3750/3750 [4:18:50<00:00,  4.14s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [12:36<00:00,  1.21s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1          1      0.995      0.975\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      11/50         0G     0.5009     0.2882     0.9108        255        640: 100%|██████████| 3750/3750 [4:19:54<00:00,  4.16s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [09:39<00:00,  1.08it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1          1      0.995      0.975\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      12/50         0G      0.491      0.284     0.9081        185        640: 100%|██████████| 3750/3750 [3:23:24<00:00,  3.25s/it]  \n",
                        "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 625/625 [12:51<00:00,  1.23s/it]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "                   all      20000     130153          1          1      0.995      0.978\n",
                        "\n",
                        "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "      13/50         0G     0.4819     0.2782     0.9081        205        640:   0%|          | 17/3750 [00:57<3:31:18,  3.40s/it]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./dataset.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ruta al archivo de configuración del dataset\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Número de épocas de entrenamiento\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tamaño de las imágenes\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Tamaño del batch\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\model.py:810\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m    809\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ultralytics\\engine\\trainer.py:389\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    385\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;241m*\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items) \u001b[38;5;241m/\u001b[39m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items\n\u001b[0;32m    386\u001b[0m     )\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;66;03m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ni \u001b[38;5;241m-\u001b[39m last_opt_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccumulate:\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\tisor\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "results = model.train(\n",
                "    data=\"./dataset.yaml\",  # Ruta al archivo de configuración del dataset\n",
                "    epochs=50,  # Número de épocas de entrenamiento\n",
                "    imgsz=640,  # Tamaño de las imágenes\n",
                "    batch=16  # Tamaño del batch\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then, we validate it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'results = model.val()'"
                        ]
                    },
                    "execution_count": 27,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "'''results = model.val()'''"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And finally, save the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'model.export(format=\"../models/captcha_yolo8_b16_e50\") '"
                        ]
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "'''model.export(format=\"../models/captcha_yolo8_b16_e50\") '''"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "'model = YOLO(\"runs/train/exp/weights/best.pt\")  # Cargar el mejor modelo entrenado\\n\\n# Realizar predicción en una imagen\\nresults = model(\"imagen_de_prueba.jpg\")\\n\\n# Mostrar resultados\\nresults.show()'"
                        ]
                    },
                    "execution_count": 29,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "'''model = YOLO(\"runs/train/exp/weights/best.pt\")  # Cargar el mejor modelo entrenado\n",
                "\n",
                "# Realizar predicción en una imagen\n",
                "results = model(\"imagen_de_prueba.jpg\")\n",
                "\n",
                "# Mostrar resultados\n",
                "results.show()'''"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.13"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
